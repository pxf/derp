
\chapter{Editor}
 
By using the editor front-end, the user creates and controls effect schematics for any type of graphical effect or behaviour by organizing components with different functionality. Each component’s behavior is described by scripts, interpreted on run-time by the engine. This feature adds to the extendability of the program, new components does not need to be compiled within the binary and can be designed and written by the users themselves.

The editor supports simple user interaction via either keyboard or mouse input; the user can manipulate component properties and preview results of either each individual output channel or the final output of the pipeline. Since the rendering is decoupled, a user can create individual effect chains and distribute these over different servers without performance decrease. 

\section{Editor Core}
In order to gain the greatest possible freedom when developing the pipeline components in terms of look and functionality, a GUI system was specially built for the editor. This opened up endless design possibilties, but ment that standard GUI widgets such as labels, text boxes and buttons had to be created. The GUI backend includes a set of fully scriptable components, that acts as a simple widget library for the editor. The GUI base can easily be ported to other application that is implemented with the PXF2 framework, providing the exact same service in any project. 

\subsection{Application and GUI Backend}
The application backend is designed in such way that it will have minimal impact on the editor. It acts more as a generic application, providing drawing and input functionality via scripting. The functionality and design can be scripted and changed/updated at runtime, which enables us to develop the editor without having to recompile the main application every time a change is made. The application backend does not have any prebuilt GUI functionality on it's own, everything is implemented via external scripts. To maintain platform independence, OpenGL is used for all rendering purposes and window handling. The scripting language chosen was Lua, due to its simple grammar and already widespread use in the gaming industry.

The focus of the backend itself is providing input/output for the scripts intuitively, without losing efficiency and speed. For example, all drawing calls are cached and only executed/rendered when explicitly needed. Ultimately, it is up to the user to specify the whole window or only a part of it when rendering is needed, and thus hopefully limiting the amount of fill needed to update the window each frame. This is accomplished inside the backend by using the stencil buffer as a mask to specify regions where to draw.

All input functionality (ie. mouse and keyboard input) are done by exposing functions from the input managing component of GLFW.

\subsection{Application Scripts}
An application script that uses our application backend only need to implement two major functions to run.
\pic[0.7]{img/application_scripts.png}{Application script body.}
\begin{enumerate}
  \item \texttt{draw()} will be called when the application needs to redraw itself, in most cases when the script itself notifies the backend using the function \texttt{needsredraw()}.
  \item \texttt{update()} is called 60 times per second, and is where the application script should do most of it's processing.
\end{enumerate}

\subsection{GUI Scripts}
We implemented a basic GUI system that was able to handle all our needs. The system is designed as a tree data structure of GUI widgets, where the root widget is the screen/view area itself.
\pic[0.7]{img/gui_system.png}{Layout of GUI system.}
\begin{enumerate}
  \item \texttt{gui:init()} has to be called to initiate the GUI system.
  \item \texttt{gui:draw()} should be called when we want to redraw the GUI. It's up to the GUI system to keep track of which widgets need to redraw.
  \item \texttt{gui:update()} should be called when we want to update the GUI. This is where mouse and keyboard inputs are handled for the GUI, and thus should be called as often as possible (preferably inside the applications \texttt{update()} function), to avoid missing any input events.
  \item \texttt{gui.widgets} is the tree of GUI widgets, i.e. the root widget instance. Any new widget added to the GUI has to be added to this one.
\end{enumerate}

Every widget type is derived from a single basic widget table, with several basic functions for drawing, hit-testing, attaching/detaching widgets etc. Due to the architecture of LUA, each of these functions are seen as virtual functions and can thus easily be overwritten to enable a specific behavior. For example, the basic draw function simply calls draw on each of the widgets attached to that widget, and nothing else. In most cases, the user might want specific graphics and can simply do so by overshadowing the draw function of the widget in the script. 

\pic[0.7]{img/gui_widgets.png}{Core components of a GUI widget.}
\begin{enumerate}
  \item \texttt{widget:draw()} will be called when the GUI system needs to redraw a specific widget. This in turn calls the \texttt{draw()} function on all the child widgets.
  \item \texttt{widget:update()} is called every time the GUI system is updated (i.e. via the \texttt{gui:update()} is run). (This in turn calls the \texttt{update()} function on all the child widgets.)
  \item \texttt{widget:addwidget(new\_child)} is used to add a new child widget to this widget in the tree.
  \item \texttt{widget.childwidgets} a list of GUI widgets that act as childs to this one. If the widget don't have any child, it will be a leaf node in the GUI tree.
  \item \texttt{widget.hitbox} a set of coordinates and dimensions of where the widget are active and should take care of mouse inputs. (The coordinates are relative to the widgets parent.)
  \item \texttt{widget.drawbox} a set of coordinates and dimensions of where the widget has its active draw area (i.e. where it is allowed to draw during the \texttt{widget:draw()} function).
\end{enumerate}

\section{Editor Layout}

\subsubsection{Overview}

\pic[0.3]{img/editor_overview.png}{\emph{Top:} Overview of the editor. \emph{Bottom:} Highlighting the different areas of the editor. \emph{1:} Workspace. \emph{2:} Toolbar. \emph{3:} Inspector view.}

The basic layout of the editor is divided in three main areas, each with specific functionality.  

\subsubsection{Workspace}
The workspace is the central area of the editor where the actual component pipeline is constructed and displayed. This is where the user creates, and otherwise manipulate the components and their connections. The user selects and adds components from a list of sub-classes from a drop-down menu, accessible by right-clicking the workspace area. 

Several workspace layouts can be loaded in memory simultaneously, which enables users to test several concepts at once, against servers with different hardware capabilities, for example. (Note, this is currently disabled as it was added quite late in the project, and is therefore not entirely bug-free.)

\subsubsection{Component Graph}
The user constructs the rendering pipeline by using a component graph. A component is a black box that performs transformations on geometry and pixel fragments, displayed as GUI widgets that contains data inputs/outputs and various properties and settings for the component itself. The user connects the component’s input and output channels together in order to control the data flow of the pipeline. 

Each component contains a table of JSON data that describes the behaviour of that specific component, by meta-data and raw shader code that is ultimately used to render the outcome of the component as a texture. The render server sends this texture for each individual pass back to the editor, together with timing information for said pass. This enables the user to view the result of each component in the editor and compare between component results or check each pixel value of a pass individually. 

The components are divided into four basic subclasses: Render, Auxiliary, Post-Process and Output respectively. Each category provides the user with an indication of the functionality of a subclass. 

\pic[0.65]{img/rendercomponent.png}{\emph{Left:} The basic render component. \emph{Right:} The advanced version of the render component, which has two extra outputs.}
The render block is the central component when rendering a 3D scene, however not required to produce an output. The prototype currently contains two simple render blocks: blinn-phong and advanced. The blinn-phong render block requires at least six standard inputs in order to render: Camera Position, Camera Lookat, Light Position, Light Color, Model and Texture, with the constraint that the last two inputs must be either Texture or Model auxiliary component blocks. The other four inputs can consist of only numeric values and can thus be connected by script blocks, static numeric blocks or basically anything that yields data in Vec3 form. For example, the camera position can be designed to follow a certain node-path calculated by a spline algorithm, a sinus function, mouse coordinates or anything you can think of that is supported by the script engine. 

The advanced render block is an extension of the blinn-phong render block, which enables access to diffuse, depth and normal values in separate texture channels from the component block. These texture channels can then be connected individually to other parts of the component graph. 

\subsubsection{Toolbar}
The toolbar contains a set of tools that manipulate the workspace. Currently, six basic tools are implemented: Undo,Redo,Move Workspace, Move Select, Rectangular Move and Delete component. The current toolset was consciously designed, in terms of both functionality and visual aspect, to mimic tools that are present in various other applications in order to quickly familiarize a user with them.

\subsubsection{Inspector}
The inspector is used to display various information about a selected component and is spawned and attached as a widget on to the inspector panel. What information to display is up to the component designer to decide, and is thus constituted by whatever the programmer want the the user to know. Currently, the inspector is used to display the result of the selected component, as well as timing information from the render pass. The inspector is one of the areas that was added in the last stages during development, and is thus the one furthest from completion. 
 