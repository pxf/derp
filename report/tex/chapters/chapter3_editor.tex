
\chapter{Editor}
 
By using our custom made editor, the user will be able to design and control a layout for any type of graphical effect or behavior. 
The editor will be able to load and store these designs directly from within the editor and distribute them to other users of this product, or preview them using the rendering backend. 
Each component’s behavior is described by scripts, interpreted on run-time by the engine. This feature adds to the extendability of the program, new components does not need to be compiled within the binary and can be designed and written by anyone with a basic understanding of programming.

The editor supports simple user interaction via either keyboard or mouse input; the user can manipulate component properties and preview results of either each individual output channel or the final output of the pipeline. Since the rendering is decoupled, a user can create individual effect chains and distribute these over different servers without severe performance decrease. 

\section{Editor Core}
To enable us to have greatest possible freedom when creating the pipeline components later on, in regards to look and functionality, a GUI system was designed for the editor.
This however ment that standard GUI widgets such as labels, text boxes and buttons had to be created. To accomplish this we decided to create our own GUI backend, where all GUI components are fully scriptable and modifiable at runtime. 

\subsection{Application and GUI Backend}
The application backend is designed in such way that it will have little to none impact on the editor functionality and look. Instead it act more as a generic application, providing drawing and input functionality via scripting. The final functionality and design can be scripted and changed/updated at runtime, which enables us to develop the editor without having to recompile the main application every time a change is made. The application backend does not have any prebuilt GUI functionality on it's own, as everything is implemented via external scripts. To keep platform independence, OpenGL is used for all rendering purposes and window handling. The scripting language chosen was Lua, due to its simple grammar and already widespread use in the gaming industry.

The focus of the backend itself was providing input/output for the scripts intuitively, without losing efficiency and speed. For example, all drawing calls are cached and only executed/rendered when explicitly needed. Ultimately, it is up to the user to specify the whole window or only a part of it when rendering is needed, and thus hopefully limiting the amount of fill needed to update the window each frame. This is accomplished inside the backend by using the stencil buffer as a mask to specify regions where to draw.

All input functionality (ie. mouse and keyboard input) are done by exposing functions from the input managing component of GLFW.

\subsection{Application Scripts}
aoeaoeaoeoa
\pic[0.7]{img/application_scripts.png}{Application script body. \textsuperscript{1)} Function that will be called when the application needs to redraw itself, usually when the script itself notifies this with \texttt{needsredraw()}.}


\subsection{GUI Scripts}

The editor is built entierly by scripts, programmed in LUA. AOEOEOEOE

-- Tree structure


-- 

Every widget type is derived from a single basic widget table, with several basic functions for drawing,hittesting,attaching/detaching widgets etc. Due to the architechture of LUA, each of these functions are seen as virtual functions and can thus easily be overwritten to enable a specific behaviour. For example, the basic draw function simply calls draw on each of the widgets attached to that widget, and nothing else. In most cases, the user might want specific graphics and can simply do so by overshadowing the draw function of the widget in the script. 

\subsection{Component Graph}
The user constructs the rendering pipeline by using a component graph. A component is a black box that performs transformations on geometry and pixel fragments, displayed as GUI widgets that contains data inputs/outputs and various properties and settings for the component itself. The user connects the component’s input and output channels together in order to control the data flow of the pipeline. 

Each component contains a table of JSON data that describes the behaviour of that specific component, by meta-data and raw shader code that is ultimately used to render the outcome of the component as a texture. The render server sends this texture for each individual pass back to the editor, together with timing information for said pass. This enables the user to view the result of each component in the editor and compare between component results or check each pixel value of a pass individually. 

The components are divided into four basic subclasses: Render, Auxiliary, Post-Process and Output respectively. Each category provides the user with an indication of the functionality of a subclass. 

Render block
The render block is the central component when rendering a 3D scene, however not required to produce an output. The prototype currently contains two simple render blocks: blinn-phong and advanced. The blinn-phong render block requires atleast six standard inputs in order to render: Camera Position, Camera Lookat, Light Position, Light Color, Model and Texture, with the constraint that the last two inputs must be either Texture or Model auxiliary component blocks. The other four inputs can consist of only numeric values and can thus be connected by script blocks, static numeric blocks or basically anything that yields data in vec3 form. For example, the camera position can be designed to follow a certain node-path calculated by a spline algorithm, a sinus function, mouse coordinates or anything you can think of that is supported by the script engine. 

The advanced render block is an extension of the blinn-phong render block, which provides the user with access to diffuse,depth  and normal values in separate texture channels from the component block. These texture channels can then be connected individually to other parts of the component graph. 

Layout/Design
The basic layout of the editor is divided in four parts/areas, each with specific functionality.  

[ lägga till någon "future versions" eller liknande?]
Originially, it was intended to include user custom color schemes, manipulated by a hue-shift shader. The purpose of which is to enable users to customize the look of the whole editor to better suit the users needs, such as light-dark contrast as well as general color preference. This idea is a common element in various modern visual editors, but will be implemented in a further version. 

Workspace
The workspace is the central area of the editor. This is where the user creates, and otherwise manipulate the components and their connections. The user selects and adds components from a list of sub-classes from a drop-down menu, accessible by right-cllicking the workspace area. 

Several workspace layouts can be loaded in memory simultaniously, wich enables users to test several concepts at once, against servers with different hardware capabilities, for example. (Note, this is currently disabled as it was added quite late in the project, and is therefore not entierly bug-free.)

Menu

Inspector
The inspector is used to display various information about a selected component, and is spawned and attached as a widget on to the inspector panel. What information to display is up to the component designer to decide, and is thus constituted by whatever the programmer finds intuitive to the user to know. Currently, the inspector is used to display the result of the selected component, as well as timing information from the render pass.

Toolbar
The toolbar contains a set of tools that manipulate the workspace. Currently, six basic tools are implemented: Undo,Redo,Move Workspace, Move Select, Rectangular Move and Delete component. However a small set, due to the flexible design of the editor, tools can easily be designed and added to the toolbar with little effort. 
 