\chapter{Results}

Basic performance test:
\begin{center}
    \begin{tabular}{ | l | l | l |} \hline
    Task count & Processor cores & Total render time \\ \hline
    2x2 & 1 & 3m 30s \\ \hline
    16x16 & 14 & 8-12s \\ \hline
    32x32 & 14 & 1m 40s \\ \hline
    \end{tabular}
    
    \emph{Total render time} is the time it takes between the job initiator from sending the job to the network and having received all the resulting tasks.
\end{center}

\section{Discussion}

% -- cant see shit from that table :c
%We can clearly see that the rendering time is spread out and varies wildly. There are
%some known problems with our task distribution algorithm that can cause an uneven distribution
%of jobs across all nodes. For our test case, optimal distribution was achieved with a region of size 16x16, yielding a render time of only a couple of seconds.

% TODO: reword the part below when we have new test data
%Increasing the region size to 32x32 quadruples the number of jobs, and shows how severe the performance hit is
%when the distribution algorithm woes. Even if the number of jobs increases, the jobs finish not much later than before on the job processor side, but instead the bottleneck seems to be on the receiving end. This has to do with the massive increase of connections the receiving application has to open and handle, one for each result it will receive.

The shading and ray tracing algorithms implemented are very simple in comparison to modern commercial solutions, and can not compete in either graphical results or speed at its current state. However, the Lightning prototype is still relevant and interesting for future developments in its field, as it tackles the problem in a new and potentially more efficient way. Currently the most relevant user groups for Lightning are hobby and 3D enthusiasts, instead of the current solutions which aim more on companies and commercial development. As hobbyists tend to have a lower budget than said companies, Lightning could become an affordable alternative. And bridge the gap in available processing power between the hobbyists and companies, as it would spread out the processing on large Lightning networks aimed just for hobbyists, by utilizing the idle processing power available by all the P2P connected hardware. Which in turn, with its combined power, could potentially compete with the commercial solutions in both quality and speed.

\section{Conclusion}
\pic[1.0]{img/conclusion.jpg}{this is not my fridge?}




\chapter{Future Work}
\section{Ray tracing algorithm}
The ray tracing and shading algorithms implemented are very naive and simple due to the time constraints in the project. If more time was given then a natural development would be to revamp the ray tracing and shading algorithms so they reach results comparable with commercial software. Another perhaps even better and more profitable approach would be to use existing ray tracing and shading solutions. This in turn would decouple the shading and clients even more, and the software could instead be viewed as a middleware between 3D modeling and shading software.
%Improved ray tracing algorithm. [More primitives, light types, blah, bleh.]

\section{SIMD Optimization}
By using SIMD (Single Instruction Multiple Data), multiple computations can be performed in parallel. If used correctly, a speedup of up to 4 times per execution thread can be expected. For us, SIMD can be applied to both ray intersection tests and data structures.

% Det här stämmer ju inte ens:
%\section{Better ways of distributing tasks}
%The current implementation just splits the tasks in half, and sends them to two separate clients (if found).
%This means that the current unhandled jobs always moves in a single chunk instead of being distributed evenly among the clients.
%A more sophisticated approach would improve the performance significantly. A good value too look at would be how much each client can process, i.e. the number of processors and how fast they are.
%The tracker should help out more with spreading the work among the available clients.

\section{Photon Mapping}
Distributed photon mapping would be a possible addition to the ray tracer, in order to get a good approximation of global illumination. The photon mapper would be implemented as a job processor, where each job could be divided into tasks such that each task would shoot a fix amount of photons. These results would then be collected at the job initiator and incorporated into the data used later on for the raytracing job processors. 

\section{Load balancing}
To further increase performance, we would need a better task distribution algorithm that accounts for CPU count and CPU load for each available node, that possible also makes decisions based on network throughput between lightning clients and the host computer.

\section{Job initiator as plugin for commercial software}
Thanks to the decoupling of job initiators and job processors/clients, it would be possible to implement a job initiator as a plugin for a commercial modeling software, ie. 3ds Max. By doing this, the usage would fit better with the current workflow of such programs, and thus more trivial for the already established community of each product/software.

\section{Handling of open network connections}
When a client has completed a subjob, it opens a new connection to the job initiator, sends the data, and then directly closes the connection.
This is very inefficient. Instead the different parts of the network should be conservative with opening new connections.

%- too much overhead?\\
%- GOOD IDEA, but only prototype. Has potential like a massive BOSS.
